1. Implementing intrinsic rewards with World Model (as simple as possible).
2. Verify whether it can learn exploration tasks. 
3. Good logging.
4. Implementing Inverse Kinematic World Model (get ready code if provided in paper).
5. Research how to compare learning performance and task performance on exploration tasks. Create good evaluation code.
6. Create learning schedule for freezing and caching weights and learning world model and policy independently.
7. Compare different results.

Optional Extra:
8. Swap Dreamer World Model with Inverse Kinematic World Model.
9. Test results compared to pixel prediction (or paper default).